{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board of the game where C- is a square with crickets (either one or five), E- is an empty space, B- is a bird that eats Lizard\n",
    "\n",
    "|C1 | E1 | E2|\n",
    "\n",
    "\n",
    "|E3 | B  | E4|\n",
    "\n",
    "\n",
    "|E4 | E5 | C5|\n",
    "\n",
    "Rewards/ penalties for stepping onto squares:\n",
    "E1-E5 = -1\n",
    "C1 = +1\n",
    "C5 = +5 -> WIN THE GAME\n",
    "B = -10 -> GAME OVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the q table\n",
    "\n",
    "rows -> C1 E1 E2 E3 B E4 E5 E6 C5\n",
    "\n",
    "\n",
    "columns -> LEFT RIGHT UP DOWN\n",
    "\n",
    "total q states -> 9*4 = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#coefficients used in the Bellman optimality equation\n",
    "alpha = 0.7\n",
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize the q table in case it is not done later\n",
    "Q_table = np.zeros((9,4))\n",
    "\n",
    "rewards = np.array([[1, -1, -1], [-1, -10, -1], [-1, -1, 5]])\n",
    "\n",
    "def reward(pos, a):\n",
    "\n",
    "    move = pos + a_to_pos(a)\n",
    "    if borders(move):\n",
    "        [x , y] = move\n",
    "        return rewards[x, y]\n",
    "    else:\n",
    "        print(\"Lizard runs into a wall!\")\n",
    "\n",
    "def pos_to_state(pos):\n",
    "    #converts the coordinates of Lizard into an encoding. E.g. [2,1] is 2*3+1 = 7 which is the 7th square counting from left to right starting from the top (the [0,0] square)\n",
    "    [x, y] = pos\n",
    "    state = 3*x + y\n",
    "    return state\n",
    "\n",
    "def state_to_pos(state):\n",
    "    #inversly convert an enocding/state into spatial coordinates\n",
    "    y = state%3\n",
    "    x = state//3\n",
    "\n",
    "    pos = np.array([x, y])\n",
    "\n",
    "    return pos\n",
    "\n",
    "def a_to_pos(a):\n",
    "    #converts a code used for an action into that action. A move is an action of moving Lizard left, right, up or down by one square\n",
    "    if a == 0:\n",
    "        return np.array([0, -1])\n",
    "    elif a == 1:\n",
    "        return np.array([0, 1])\n",
    "    elif a == 2:\n",
    "        return np.array([-1, 0])\n",
    "    elif a == 3:\n",
    "        return np.array([1, 0])\n",
    "    else:\n",
    "        print(\"error, wrong action. Out of [0,3] bounds\")\n",
    "\n",
    "def borders(pos):\n",
    "    #checks if such position is possible in the Lizard game\n",
    "    [x , y] = pos\n",
    "\n",
    "    if (2>=x>=0) and (2>=y>=0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def max_reward(s_new):\n",
    "    #returns the maximal reward that can be obtained by performing a single action from a given state\n",
    "    pos = state_to_pos(s_new)\n",
    "    if borders(pos+ a_to_pos(0)):\n",
    "        r_left = reward(pos, 0)\n",
    "    else:\n",
    "        r_left = 0\n",
    "    if borders(pos+ a_to_pos(1)):\n",
    "        r_right = reward(pos, 1)\n",
    "    else:\n",
    "        r_right = 0\n",
    "    if borders(pos+ a_to_pos(2)):\n",
    "        r_up = reward(pos, 2)\n",
    "    else:\n",
    "        r_up = 0\n",
    "    if borders(pos+ a_to_pos(3)):\n",
    "        r_down = reward(pos, 3)\n",
    "    else:\n",
    "        r_down = 0\n",
    "\n",
    "    next_reward = max(r_left, r_right, r_up, r_down)\n",
    "\n",
    "    return next_reward\n",
    "\n",
    "def update_q_table(pos, a):\n",
    "    #updates the Q_table\n",
    "    global Q_table, alpha, gamma\n",
    "    s = pos_to_state(pos)\n",
    "    s_next = pos_to_state(pos + a_to_pos(a))\n",
    "    \n",
    "    #Bellman optimality equation\n",
    "    Q_table[s, a] = (1 - alpha) * Q_table[s, a] + alpha * (reward(pos, a) + gamma * max_reward(s_next))\n",
    "\n",
    "\n",
    "\n",
    "def show_game(pos):\n",
    "    #a simple way of displaying the current state of the game\n",
    "    [x, y] = pos\n",
    "    board = list(\"â¬œâ¬œâ¬œ\\nâ¬œðŸ”¶â¬œ\\nâ¬œâ¬œðŸ”·\")\n",
    "    if x == 0:\n",
    "        board[y] = 'â¬›'\n",
    "    if x == 1:\n",
    "        board[4+y] = 'â¬›'\n",
    "    if x == 2:\n",
    "        board[8+y] = 'â¬›'\n",
    "    \n",
    "    board = \"\".join(board)\n",
    "\n",
    "    return board\n",
    "\n",
    "\n",
    "def train_Lizard(n, start = np.array([2,0]), prompts = False, visualize = False):\n",
    "    #runs the game n number of times\n",
    "    global Q_table\n",
    "\n",
    "    for i in range(n): #specifies how many times the game should be played (until a win or loss)\n",
    "        over = True\n",
    "        #at the beginning of the game the lizard always starts in the same position\n",
    "        pos = start\n",
    "\n",
    "        if prompts:\n",
    "            print(\"starting a new game\")\n",
    "\n",
    "        while over:\n",
    "            #check if the action is allowed\n",
    "\n",
    "            a = random.randint(0,3)\n",
    "            while not borders(pos + a_to_pos(a)):\n",
    "                a = random.randint(0,3)\n",
    "\n",
    "            #past this moment an ALLOWED action is generated\n",
    "            if prompts:\n",
    "                print(\"new action generated: \"+str(a))\n",
    "            update_q_table(pos, a)\n",
    "            #the Q-table gets updated\n",
    "            if prompts:\n",
    "                print(\"the table is succesfully updated\")\n",
    "            pos = pos + a_to_pos(a)\n",
    "            #the lizard finally moves\n",
    "\n",
    "            #check if we win or lose\n",
    "            if (pos == np.array([1,1])).all():\n",
    "                over = False\n",
    "                if prompts:\n",
    "                    print(\"loss\")\n",
    "            if (pos == np.array([2, 2])).all():\n",
    "                over = False\n",
    "                if prompts:\n",
    "                    print(\"win\")\n",
    "            \n",
    "            if visualize:\n",
    "                print(show_game(pos))\n",
    "                print(\"------\")\n",
    "\n",
    "def play_game(pos):\n",
    "    #let Lizard play the game\n",
    "    global Q_table\n",
    "\n",
    "    turn = 0\n",
    "\n",
    "    while not (pos == np.array([2,2])).all():\n",
    "        print(show_game(pos))\n",
    "        print(\"-------\")\n",
    "        if borders(pos + a_to_pos(np.argmax(Q_table[pos_to_state(pos), :]))):\n",
    "            #if the action is allowed then Lizard. Action is selected by taking the index of the highest value for a row in the state-action Q_table.\n",
    "            pos = pos + a_to_pos(np.argmax(Q_table[pos_to_state(pos), :]))\n",
    "        turn = turn + 1\n",
    "        if turn == 6:\n",
    "            #terminate the game after 6 moves no matter what\n",
    "            break\n",
    "    \n",
    "    print(show_game(pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train Lizard\n",
    "Q_table = np.zeros((9,4))\n",
    "\n",
    "train_Lizard(10) #Lizard is trained just on 10 games!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Q_table looks like this after training:\n",
      "[[  0.          -0.19838      0.           0.        ]\n",
      " [  0.7          0.           0.         -10.5084    ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.         -10.5084       0.973       -0.7       ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           2.99994095  -0.19995626   0.        ]\n",
      " [ -0.99757      3.5        -10.5084       0.        ]\n",
      " [  0.           0.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Q_table looks like this after training:\\n\"+str(Q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if Lizard now knows how to play to win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬œâ¬œâ¬œ\n",
      "â¬œðŸ”¶â¬œ\n",
      "â¬›â¬œðŸ”·\n",
      "-------\n",
      "â¬œâ¬œâ¬œ\n",
      "â¬œðŸ”¶â¬œ\n",
      "â¬œâ¬›ðŸ”·\n",
      "-------\n",
      "â¬œâ¬œâ¬œ\n",
      "â¬œðŸ”¶â¬œ\n",
      "â¬œâ¬œâ¬›\n"
     ]
    }
   ],
   "source": [
    "start = np.array([2,0])\n",
    "play_game(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
